{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bed6cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "728b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction donnee\n",
    "def f(x):\n",
    "    return (x[0]-5)**2 + (x[1] - 1)**2 + (x[2]+2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f586e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient de la fonction (Градиент функции f)\n",
    "def gradf(f,x,h = 1e-5):\n",
    "    df = []\n",
    "    n = len(x) # Nombre de variable\n",
    "    \n",
    "    for i in range(n):\n",
    "        x_tmp = copy.copy(x)\n",
    "        x_tmp[i] += h\n",
    "        dfi = (f(x_tmp) - f(x)) / h\n",
    "        df.append(dfi)\n",
    "    return np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eff42ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.99999, -1.99999,  4.00001])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: gradient de f au point [0,0,0]\n",
    "gradf(f,x=[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ceca63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Одномерная оптимизация\n",
    "# La methode de Newton permet de minimiser la fonction d'une variable\n",
    "def newton_method(f,x0=0,eps=1e-5):\n",
    "    x = x0\n",
    "    h = 1e-5 # le pas du differentiel\n",
    "    while True:\n",
    "        df = (f(x+h) - f(x))/h # 1ere derivee (первая производная)\n",
    "        d2f = (f(x+2*h) + f(x) -2*f(x+h)) / h**2 # 2eme derivee (вторая производная)\n",
    "        \n",
    "        if np.abs(df) < eps:\n",
    "            break\n",
    "        \n",
    "        x = x - df/d2f\n",
    "    \n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8528cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fonction a une variable\n",
    "def g(x):\n",
    "    return (x-2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48617084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.999995\n"
     ]
    }
   ],
   "source": [
    "print(newton_method(g,x0=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de71a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastestGradientDescent(f, x0=[0,0,0], tmin=0.01, tmax=10, eps1=0.01, eps2=0.01, kmax=400): # Шаг 1\n",
    "    k = 0 # Iteration\n",
    "    x = np.array(x0)\n",
    "    \n",
    "    \n",
    "    gradf_x = gradf(f,x) # Шаг 2 (on calcule le gradient)\n",
    "\n",
    "    # On definit la fonction phi (voir etape 2 de l'algorithm)\n",
    "    def phi(t):\n",
    "        return f(x + t * gradf(f,x))\n",
    "  \n",
    "    while k != kmax: #(Si on n'a pas atteint le nombre maximal de tentatives)\n",
    "        \n",
    "        if (np.sqrt(np.sum(gradf_x**2)) < eps1): # Шаг 3 (2eme condition, voir etape 3)\n",
    "            break\n",
    "        \n",
    "        k += 1\n",
    "        best_t = newton_method(phi) # Шаг 4 (решаем одномерную задачу оптимизации)\n",
    "        \n",
    "        # On verifie que t appartient a [tmin tmax]\n",
    "        if abs(best_t) < tmin:\n",
    "            best_t = np.sign(best_t) * tmin\n",
    "        elif abs(best_t) > tmax:\n",
    "            best_t = np.sign(best_t) * tmax\n",
    "        \n",
    "        x_prev = copy.copy(x)\n",
    "        \n",
    "        x = x + best_t*gradf_x # Щаг 5 (on met a mise notre x)\n",
    "        \n",
    "        if (np.sqrt(np.sum((x_prev - x)**2)) < eps2): # Шаг 6 (3eme condition, voir etape 6)\n",
    "            break\n",
    "                \n",
    "        gradf_x = gradf(f,x)\n",
    "        \n",
    "    return x, f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6878d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmin:  [ 4.999995    0.99999501 -2.00000029]\n",
      "fmin:  5.001587661767303e-11\n"
     ]
    }
   ],
   "source": [
    "xmin, fmin = fastestGradientDescent(f,x0=[2,2,-1])\n",
    "print(\"xmin: \",xmin)\n",
    "print(\"fmin: \", fmin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
